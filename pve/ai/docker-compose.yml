version: '3.8'

networks:
  homelab:
    driver: bridge
  ai:
    external: true
  ollama:
    external: true

services:
  chatgpt-next-web:
    image: yidadaa/chatgpt-next-web
    container_name: chatgpt-next-web
    networks:
      - homelab
      - ai
    restart: unless-stopped
    labels:
      - com.centurylinklabs.watchtower.enable=true
      - homepage.group=AI
      - homepage.name=chatgpt-next-web
      - homepage.icon=https://cdn.jsdelivr.net/gh/ChatGPTNextWeb/ChatGPT-Next-Web@main/docs/images/icon.svg
      - homepage.href=http://${DEPLOY_HOST:-127.0.0.1}:3006
      - homepage.description=openai 客户端
    environment:
      TZ: Asia/Shanghai
      CODE: "${CNW_CODE}" # 使用的code
      OPENAI_API_BASE_URL: ${OPENAI_API_BASE_URL} # cloudflare 代理地址
      OPENAI_API_KEY: ${OPENAI_API_KEY:-sk} # openai api key
    ports:
      - '3006:3000'
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 3s
        order: stop-first
      restart_policy:
        condition: on-failure
        max_attempts: 1
  dockerproxy:
    image: ghcr.io/tecnativa/docker-socket-proxy:latest
    container_name: dockerproxy
    restart: unless-stopped
    networks:
      - homelab
    environment:
      - CONTAINERS=1 # Allow access to viewing containers
      - SERVICES=0 # Allow access to viewing services (necessary when using Docker Swarm)
      - TASKS=0 # Allow access to viewing tasks (necessary when using Docker Swarm)
      - POST=0 # Disallow any POST operations (effectively read-only)
    ports:
      - "${DOCKER_PROXY_HOST:-127.0.0.1}:2575:2375"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro # Mounted as read-only
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 3s
        order: stop-first
      restart_policy:
        condition: on-failure
        max_attempts: 1
  dufs:
    image: sigoden/dufs
    container_name: dufs
    networks:
      - homelab
    restart: unless-stopped
    command: '/data -A -a shelken:${DUFS_PASSWORD}@/:rw'
    labels:
      - com.centurylinklabs.watchtower.enable=true
      - homepage.group=Dufs
      - homepage.name=AI文件
      - homepage.icon=https://assets.shelken.top/ali/PicGo/2023-04/8255d189.ico
      - homepage.href=http://${DEPLOY_HOST:-127.0.0.1}:5000
      - homepage.description=AI文件, 包含了/data目录下的文件
    environment:
      TZ: Asia/Shanghai
    volumes:
      - "${DUFS_DATA_DIR}:/data"
    ports:
      - '5000:5000'
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 3s
        order: stop-first
      restart_policy:
        condition: on-failure
        max_attempts: 1
  geminiprochat:
    image: ghcr.io/babaohuang/geminiprochat:main
    container_name: geminiprochat
    networks:
      - homelab
      - ai
    restart: unless-stopped
    labels:
      - com.centurylinklabs.watchtower.enable=true
      - homepage.group=AI
      - homepage.name=geminiprochat
      - homepage.icon=https://cdn.jsdelivr.net/gh/babaohuang/GeminiProChat@raw/main/public/icon.svg
      - homepage.href=http://${DEPLOY_HOST:-127.0.0.1}:3016
      - homepage.description=谷歌gemini对话
    environment:
      TZ: Asia/Shanghai
      GEMINI_API_KEY: ${GEMINI_API_KEY} # gemini api key
      API_BASE_URL: ${GEMINI_API_BASE_URL} # cloudflare 代理地址
    ports:
      - '3016:3000'
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 3s
        order: stop-first
      restart_policy:
        condition: on-failure
        max_attempts: 1
  ollama-telegram:
    container_name: ollama-telegram
    build: 
      context: ${OLLAMA_BOT_BUILD_PATH}
      dockerfile: dockerfile
    networks:
      - homelab
      - ai
      - ollama
    restart: unless-stopped
    labels:
      - com.centurylinklabs.watchtower.enable=flase
      - homepage.group=AI
      - homepage.name=ollama-telegram
      - homepage.icon=https://cdn.jsdelivr.net/gh/ruecat/ollama-telegram@main/res/github/ollama-telegram-readme.png
      # - homepage.href=http://${DEPLOY_HOST:-127.0.0.1}:11435
      - homepage.description=ollama-telegram-bot
    environment:
      TZ: Asia/Shanghai
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL}
      INITMODEL: "llama2-chinese:latest"
      USER_IDS: "${OLLAMA_BOT_USER_IDS}"
      ADMIN_IDS: "${OLLAMA_BOT_ADMIN_IDS}"
      TOKEN: "${OLLAMA_BOT_TOKEN}"
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 3s
        order: stop-first
      restart_policy:
        condition: on-failure
        max_attempts: 1
  ollama-webui:
    image: ghcr.io/ollama-webui/ollama-webui:main
    container_name: ollama-webui
    networks:
      - homelab
      - ai
      - ollama
    restart: unless-stopped
    labels:
      - com.centurylinklabs.watchtower.enable=true
      - homepage.group=AI
      - homepage.name=ollama-webui
      - homepage.icon=https://cdn.jsdelivr.net/gh/ollama-webui/ollama-webui@main/static/favicon.png
      - homepage.href=http://${DEPLOY_HOST:-127.0.0.1}:11435
      - homepage.description=ollama-webui
    environment:
      TZ: Asia/Shanghai
    extra_hosts:
      - "host.docker.internal:${OLLAMA_BASE_URL:-127.0.0.1}"
    ports:
      - '11435:8080'
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 3s
        order: stop-first
      restart_policy:
        condition: on-failure
        max_attempts: 1
  watchtower:
    image: containrrr/watchtower
    container_name: watchtower
    networks:
      - homelab
    restart: unless-stopped
    command: '-c --interval 3600 --label-enable --cleanup'
    labels:
      - com.centurylinklabs.watchtower.enable=true
      - homepage.group=未分类
      - homepage.name=watchtower-ai
      - homepage.icon=watchtower.png
      - homepage.description=watchtower
    environment:
      TZ: Asia/Shanghai
      WATCHTOWER_NOTIFICATIONS: slack
      WATCHTOWER_NOTIFICATION_SLACK_HOOK_URL: ${WATCHTOWER_SLACK_HOOK_URL}
      WATCHTOWER_NOTIFICATION_SLACK_IDENTIFIER: ${WATCHTOWER_SLACK_IDENTIFIER}
      WATCHTOWER_NOTIFICATION_SLACK_CHANNEL: ${WATCHTOWER_SLACK_CHANNEL}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    deploy:
      replicas: 1
      update_config:
        parallelism: 1
        delay: 3s
        order: stop-first
      restart_policy:
        condition: on-failure
        max_attempts: 1
